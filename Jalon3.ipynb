{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Application Streamlit\n"
      ],
      "metadata": {
        "id": "62K7H1t8l_-d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xX1qVjBvkV-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84de5f12-d202-4c58-b0c4-3be029a7cdb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TfidfVectorizer(max_df=0.6, min_df=0.008)\n",
            "NMF(n_components=15)\n"
          ]
        }
      ],
      "source": [
        "#Importation des fichiers pickle\n",
        "\n",
        "import pandas as pd\n",
        "from pickle import *\n",
        "file_name1 = open(\"/content/drive/MyDrive/IAA2/Sitou/vectorizer.pkl\",'rb')\n",
        "vectorizer = load(file_name1)\n",
        "\n",
        "file_name2 = open(\"/content/drive/MyDrive/IAA2/Sitou/nmf_model.pkl\",\"rb\")\n",
        "model_pred = load(file_name2)\n",
        "print(vectorizer)\n",
        "print(model_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoaL1gDrBETQ",
        "outputId": "5a45f646-4fb1-4e5b-ebd6-c44242e27448"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 8.9 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 52.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pré-traitement \n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import contractions\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    text_processed = \" \".join(tokenizer.tokenize(text))\n",
        "    return text_processed\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Er6JAtpI_Qj3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \n",
        "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    lemmatized_text_list = list()\n",
        "    \n",
        "    for word, tag in tokens_tagged:\n",
        "        if tag.startswith('J'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'a')) # Lemmatise adjectives. Not doing anything since we remove all adjective\n",
        "        elif tag.startswith('V'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'v')) # Lemmatise verbs\n",
        "        elif tag.startswith('N'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'n')) # Lemmatise nouns\n",
        "        elif tag.startswith('R'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'r')) # Lemmatise adverbs\n",
        "        else:\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatisation\n",
        "    \n",
        "    return \" \".join(lemmatized_text_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIhoLWVz_iWG",
        "outputId": "3e90a813-1768-458f-cd43-10329ce8d451"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/language.py:1899: UserWarning: [W123] Argument disable with value ['parser', 'tagger', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
            "  config_value=config[\"nlp\"][key],\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    return \" \".join([word.lower() for word in text.split()])"
      ],
      "metadata": {
        "id": "1m4EqhwL_lLn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contraction_text(text):\n",
        "    return contractions.fix(text)"
      ],
      "metadata": {
        "id": "jESLPyzT_rK3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = ['not', 'no', 'never', 'nor', 'hardly', 'barely']\n",
        "negative_prefix = \"NOT_\"\n",
        "\n",
        "def get_negative_token(text):\n",
        "    tokens = text.split()\n",
        "    negative_idx = [i+1 for i in range(len(tokens)-1) if tokens[i] in negative_words]\n",
        "    for idx in negative_idx:\n",
        "        if idx < len(tokens):\n",
        "            tokens[idx]= negative_prefix + tokens[idx]\n",
        "    \n",
        "    tokens = [token for i,token in enumerate(tokens) if i+1 not in negative_idx]\n",
        "    \n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "HiEwHAdA_uzW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    english_stopwords = stopwords.words(\"english\") + list(STOP_WORDS) + [\"tell\", \"restaurant\"]\n",
        "    \n",
        "    return \" \".join([word for word in text.split() if word not in english_stopwords])\n"
      ],
      "metadata": {
        "id": "FszEzG2a_1PO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \n",
        "    # Tokenize review\n",
        "    text = tokenize_text(text)\n",
        "    \n",
        "    # Lemmatize review\n",
        "    text = lemmatize_text(text)\n",
        "    \n",
        "    # Normalize review\n",
        "    text = normalize_text(text)\n",
        "    \n",
        "    # Remove contractions\n",
        "    text = contraction_text(text)\n",
        "\n",
        "    # Get negative tokens\n",
        "    text = get_negative_token(text)\n",
        "    \n",
        "    # Remove stopwords\n",
        "    text = remove_stopwords(text)\n",
        "    \n",
        "    return text"
      ],
      "metadata": {
        "id": "lpBLGKpjADZf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqSJutwCP1V7",
        "outputId": "0c7ecec1-3009-4694-cf94-79ed64510ae8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fonction de prédiction\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "def prediction(model,vectorizer,n_topic,new_reviews):\n",
        "  new_reviews=preprocess_text(new_reviews)\n",
        "  blob=TextBlob(new_reviews)\n",
        "  sentimentBlob=blob.sentiment.polarity\n",
        "  new_reviews = [new_reviews]\n",
        "  new_reviews_transformed=vectorizer.transform(new_reviews)\n",
        "\n",
        "  prediction= model.transform(new_reviews_transformed)\n",
        " \n",
        "  topics=['Esthétique du cadre','Qualité de la sauce','Qualité de la pizza','Qualité du service au niveau de la prise de commande','Rapidité du sercive','Staff du restaurant','Description des burgers',\n",
        "\"Temps d'attente\",\"Menu poulet\",\"Boissons disponibles au bar\",\"Comparaison par rapport à un autre passage dans le restaurant\",\"Plainte des clients au manager\",\"Garniture sandwich\",\"Menu sushi\",\n",
        "  \"Probabilité de revenir dans le restaurant\"]\n",
        "  if sentimentBlob<0 and sentimentBlob>-1:\n",
        " \n",
        "    max = np.argsort(prediction)\n",
        "    max_list=(list(max[0]))\n",
        "    max_list.reverse()\n",
        "    print(max_list)\n",
        "    topic=[]\n",
        "    for i in range(n_topic):\n",
        "      topic.append(topics[max_list[i]])  \n",
        "    return sentimentBlob,prediction,topic\n",
        "\n",
        "  return sentimentBlob\n",
        "new_reviews = \"I dont like chicken \"\n",
        "prediction(model_pred,vectorizer,2,new_reviews)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu66GdA6o5jy",
        "outputId": "728d021a-59b8-447e-e996-a6f1949fb4a9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8, 14, 13, 12, 11, 10, 9, 7, 6, 5, 4, 3, 2, 1, 0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.6, array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.13364052, 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ]]), ['Menu poulet',\n",
              "  'Probabilité de revenir dans le restaurant'])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}